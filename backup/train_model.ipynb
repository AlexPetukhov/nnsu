{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Untitled15.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOM+3KxwG70gLFfWg0nh7FK"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "D-e5kdMhLq42"
   },
   "source": [
    "import keras\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "import keras.layers as L\n",
    "import os, csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hoN8X-6IEahY"
   },
   "source": [
    "filter_means = np.array([32, 32, 256, 256, 256, 512, 512, 512, 512])\n",
    "filter_means_2 = filter_means"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dLcaYVYmXYzf"
   },
   "source": [
    "def get_conv_model(filter_means, input_layer):\n",
    "  for i in range(len(filter_means)):\n",
    "    if i == 0:\n",
    "      conv = L.Conv1D(filters=filter_means[i], kernel_size=9, padding='same', strides=5, activation='relu')(input_layer)\n",
    "    else:\n",
    "      conv = L.Conv1D(filters=filter_means[i], kernel_size=9, padding='same', strides=5, activation='relu')(relu)\n",
    "    drop = L.Dropout(0.25)(conv)\n",
    "    conv = L.Conv1D(filters=filter_means[i], kernel_size=9, padding='same', strides=5)(drop)\n",
    "    batch = L.BatchNormalization()(conv)\n",
    "    #pool = L.MaxPool1D(2)(batch)\n",
    "    relu = L.ReLU()(batch)\n",
    "    drop = L.Dropout(0.25)(relu)\n",
    "  #return L.Lambda(lambda x: L.K.batch_flatten(x))(relu)\n",
    "  return L.Flatten()(drop)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jQ5sEFeNJWq5"
   },
   "source": [
    "def model_many_diseases2():\n",
    "  input_layer = L.Input(shape=(4500, 12))\n",
    "  input_layer2 = L.Input(shape=(4500, 4))\n",
    "  input_layer3 = L.Input(shape=(60,))\n",
    "\n",
    "  model_12 = get_conv_model(filter_means[:2], input_layer)\n",
    "  model_4 = get_conv_model(filter_means_2[:2], input_layer2)\n",
    "  \n",
    "  merged = L.Concatenate(axis=1)([model_12, model_4, input_layer3])\n",
    "\n",
    "  dense_out1 = L.Dense(512, activation='relu')(merged)\n",
    "  dense_out2 = L.Dense(9)(dense_out1)\n",
    "  \n",
    "  model = keras.Model([input_layer,input_layer2, input_layer3], [dense_out2])\n",
    "  return model"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C-e8xzcXMF8Q"
   },
   "source": [
    "model = model_many_diseases2()"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m4XhKgk9MDjW"
   },
   "source": [
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_features_from_signal(signal):\n",
    "    signal = signal.T\n",
    "    features = np.empty(60)\n",
    "    features[:12] = signal.max(axis=1)\n",
    "    features[12:24] = signal.min(axis=1)\n",
    "    features[24:36] = signal.mean(axis=1)\n",
    "    features[36:48] = signal.std(axis=1)\n",
    "    features[48:] = signal.max(axis=1) + signal.min(axis=1)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9kJUIHPGL-l9"
   },
   "source": [
    "def generator_st(x_train, y_train, batch_size):\n",
    "    # Create empty arrays to contain batch of features and label\n",
    "\n",
    "    batch_features_1 = np.zeros((batch_size, 4500, 12))\n",
    "    batch_features_2 = np.zeros((batch_size, 4500, 4))\n",
    "    batch_features_3 = np.zeros((batch_size, 60))\n",
    "    batch_labels = np.zeros(batch_size, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index = np.random.choice(len(x_train))\n",
    "            x = x_train[index][:, :12]\n",
    "            delin = x_train[index][:, 12:]\n",
    "            shift = np.random.randint(0, len(x) - 4500 + 1, 1)[0]\n",
    "\n",
    "            batch_features_1[i] = x[shift : shift + 4500]\n",
    "            batch_features_2[i] = delin[shift : shift + 4500]\n",
    "            batch_features_3[i, :12] = batch_features_1[i].max(axis=0)\n",
    "            batch_features_3[i, 12:24] = batch_features_1[i].min(axis=0)\n",
    "            batch_features_3[i, 24:36] = batch_features_1[i].mean(axis=0)\n",
    "            batch_features_3[i, 36:48] = batch_features_1[i].std(axis=0)\n",
    "            batch_features_3[i, 48:] = batch_features_1[i].max(axis=0) + batch_features_1[i].min(axis=0)\n",
    "            batch_labels[i] = y_train[index]\n",
    "        yield [batch_features_1, batch_features_2, batch_features_3], batch_labels"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def load_data(record_path, delineation_path):\n",
    "    data = sio.loadmat(record_path)\n",
    "    signal = data['ECG']['data'][0][0]\n",
    "    delineation = np.load(delineation_path) / 5\n",
    "    return signal.T, delineation.T\n",
    "\n",
    "def get_train_data(data_and_delin_path, mat_folder, delin_folder):\n",
    "    data_path = os.path.join(data_and_delin_path, mat_folder)\n",
    "    data_list = os.listdir(data_path)\n",
    "    delin_path = os.path.join(data_and_delin_path, delin_folder)\n",
    "    delin_list = os.listdir(delin_path)\n",
    "    if 'REFERENCE.csv' in data_list:\n",
    "        data_list.remove('REFERENCE.csv')\n",
    "    if '.DS_Store' in data_list:\n",
    "        data_list.remove('.DS_Store')\n",
    "    if 'REFERENCE.csv' in delin_list:\n",
    "        delin_list.remove('REFERENCE.csv')\n",
    "    if '.DS_Store' in delin_list:\n",
    "        delin_list.remove('.DS_Store')\n",
    "    data_list.sort()\n",
    "    delin_list.sort()\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    reference_dict = {}\n",
    "    reference_path = os.path.join(data_path, 'REFERENCE.csv')\n",
    "    reader = pd.read_csv(reference_path)\n",
    "    for i, row in reader.iterrows():\n",
    "        reference_dict[row['Recording']] = int(row['First_label'])\n",
    "\n",
    "    for data, delin in zip(data_list, delin_list):\n",
    "        cur_data_path = os.path.join(data_path, data)\n",
    "        cur_delin_path = os.path.join(delin_path, delin)\n",
    "        signal, delineation = load_data(cur_data_path, cur_delin_path)\n",
    "        x_train.append(np.concatenate((signal, delineation), axis=1))\n",
    "        data_name = data.split('.')[0]\n",
    "        y_train.append(reference_dict[data_name] - 1)\n",
    "    return x_train, y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_pmhYl5XL-ok"
   },
   "source": [
    "# x_train = np.random.rand(500, 5000, 16)\n",
    "# y_train = np.random.randint(0, 9, 500)\n",
    "\n",
    "data_and_delin_path = 'DATA'\n",
    "mat_folder = 'validation_set'\n",
    "delin_folder = 'delineation_leads_val'\n",
    "x_train, y_train = get_train_data(data_and_delin_path, mat_folder, delin_folder)\n",
    "\n",
    "X_val = np.load(os.path.join(data_and_delin_path, 'X.npy'))\n",
    "y_val = np.load(os.path.join(data_and_delin_path, 'y.npy'))\n",
    "name_val = np.load(os.path.join(data_and_delin_path, 'name.npy'))\n",
    "X_features = np.array([get_features_from_signal(x) for x in X_val[:,:,:12]])\n",
    "\n",
    "history = model.fit(generator_st(x_train=x_train, y_train=y_train, batch_size=128),\n",
    "                    epochs=100, steps_per_epoch=8,\n",
    "                    validation_data=([X_val[:, :, :12], X_val[:, :, 12:], X_features], y_val))\n",
    "\n",
    "\n",
    "model.save(\"models/trained_model\")\n",
    "model.save(\"models/trained_model_h5.h5\")"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 439ms/step - loss: 2.3164 - accuracy: 0.1821 - val_loss: 2.0891 - val_accuracy: 0.2425\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "INFO:tensorflow:Assets written to: models/trained_model/assets\n"
     ]
    }
   ]
  }
 ]
}